plot <- ggplot(df_filtered, aes(x = longitude, y = latitude, color = factor(cluster))) +
geom_point(size = 3) +
scale_color_manual(values = c("gray", "blue", "green", "red", "orange","pink","black")) +
labs(title = "Clusters", x = "Longitude", y = "Latitude") +
theme_minimal()
plot
cluster_counts <- table(df$cluster)
sorted_clusters <- sort(cluster_counts, decreasing = TRUE)
top_cluster <- names(sorted_clusters)[1]
print(top_cluster)
print(head(core_samples_mask, 10))
cluster_counts <- table(df$cluster)
print(cluster_counts)
df_top_cluster <- df_eq_sel_range[df_eq_sel_range$cluster == cluster_mask, ]
map <- leaflet() %>%
addTiles() %>%
setView(lng = -120, lat = 38, zoom = 5)
map <- map %>%
addCircleMarkers(data = df_top_cluster,
lng = ~longitude,
lat = ~latitude,
radius = ~mag/0.05,
color = "red",
opacity = 0.5)
map
df_top_cluster
library("data.table")
library("stats")
library("forecast")
library("dplyr")
library("vars")
file_path <- "C:\\Users\\Akhil_kamuni\\OneDrive\\Desktop\\MP_2\\MP_2\\datasets\\combined_eq_california_timeseries.csv"
df_eq <- read.csv(file_path)
df_eq <- df_eq[, c("time", "mag", "sig", "longitude", "latitude", "depth")]
df_eq$time <- as.POSIXct(df_eq$time)
df_eq$timestamps <- df_eq$time
head(df_eq)
str(df_eq)
duplicated_rows <- df_eq[duplicated(df_eq$timestamps), ]
df_eq <- df_eq[!duplicated(df_eq$timestamps), ]
# Calculating difference between consecutive timestamps
timestamps_diff <- diff(df_eq$timestamps)
summary(timestamps_diff)
str(df_eq)
library(zoo)
# Number 1: Time intervals between consecutive earthquakes
df_eq$time_diff <- c(NA, diff(df_eq$timestamps))
df_eq$time_diff_float <- as.numeric(df_eq$time_diff, units = "secs")
# Number 2: Rolling mean of magnitudes from the last 10 earthquakes
df_eq$mag_roll_10 <- rollapplyr(df_eq$mag, width = 10, FUN = mean, fill = NA,
align = "right")
# Drop rows with NA values
df_eq <- na.omit(df_eq)
summary(df_eq)
filtered_rows <- df_eq[df_eq$time_diff_float > 86400, ]
num_filtered_rows <- nrow(filtered_rows)
print(num_filtered_rows)
# Filter rows based on the condition time_diff_float > 86400 (which means time difference greater than 86400 seconds)
filtered_rows <- subset(df_eq, time_diff_float > 86400)
print(filtered_rows)
min_date <- min(df_eq$time)
min_date <- as.Date(min_date)
print(min_date)
max_date <- max(df_eq$time)
max_date <- as.Date(max_date)
print(max_date)
max_date <- max(df_eq$time)
max_date <- as.Date(max_date)
print(typeof(max_date))
start_date <- min(df_eq$time)
# Calculate the number of days between min_date and max_date
number_of_days <- as.integer(difftime(max(df_eq$time), min(df_eq$time), units = "days"))
print(paste("number_of_days:", number_of_days))
date_list <- seq(start_date, by = "days", length.out = number_of_days)
print(date_list[2])
print(class(date_list[2]))
start_date <- min(df_eq$time)
number_of_days <- as.integer(difftime(max(df_eq$time), min(df_eq$time), units = "days"))
print(paste("number_of_days:", number_of_days))
date_list <- seq(start_date, by = "days", length.out = number_of_days)
print(date_list[2])
print(class(date_list[2]))
df_eq
setDT(df_eq)
setkey(df_eq, time)
rownames(df_eq) <- df_eq$time
setkey(df_eq, time)
library(zoo)
# Aggregate data by day
df_eq[, date := as.IDate(time)]
df_daily <- df_eq[, .(
mag_max = max(mag, na.rm = TRUE),
event_count = .N,
mag_mean = mean(mag, na.rm = TRUE),
mag_sum = sum(mag, na.rm = TRUE),
mag_scatter = sd(mag, na.rm = TRUE),
longitude_mean = mean(longitude, na.rm = TRUE),
longitude_std = sd(longitude, na.rm = TRUE),
latitude_mean = mean(latitude, na.rm = TRUE),
latitude_std = sd(latitude, na.rm = TRUE),
depth_mean = mean(depth, na.rm = TRUE),
depth_std = sd(depth, na.rm = TRUE),
time_diff_float_mean = mean(time_diff_float, na.rm = TRUE),
time_diff_float_std = sd(time_diff_float, na.rm = TRUE)
), by = date]
# Calculate the rolling mean for the mag_mean column with a window of 10 days
df_daily[, mag_roll_10 := rollapply(mag_mean, width = 10, FUN = mean, fill = NA, align = "right")]
df_daily
library(data.table)
library(ggplot2)
library(naniar)
df_daily <- as.data.frame(df_daily)
ggplot_missing <- vis_miss(df_daily) +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
labs(title = "Heatmap of Missing Values")
print(ggplot_missing)
library("data.table")
library("stats")
library("forecast")
library("dplyr")
library("vars")
file_path <- "C:\\Users\\Akhil_kamuni\\OneDrive\\Desktop\\MP_2\\MP_2\\datasets\\combined_eq_california_timeseries.csv"
df_eq <- read.csv(file_path)
df_eq <- df_eq[, c("time", "mag", "sig", "longitude", "latitude", "depth")]
df_eq$time <- as.POSIXct(df_eq$time)
df_eq$timestamps <- df_eq$time
head(df_eq)
str(df_eq)
duplicated_rows <- df_eq[duplicated(df_eq$timestamps), ]
df_eq <- df_eq[!duplicated(df_eq$timestamps), ]
# Calculating difference between consecutive timestamps
timestamps_diff <- diff(df_eq$timestamps)
summary(timestamps_diff)
str(df_eq)
library(zoo)
# Number 1: Time intervals between consecutive earthquakes
df_eq$time_diff <- c(NA, diff(df_eq$timestamps))
df_eq$time_diff_float <- as.numeric(df_eq$time_diff, units = "secs")
# Number 2: Rolling mean of magnitudes from the last 10 earthquakes
df_eq$mag_roll_10 <- rollapplyr(df_eq$mag, width = 10, FUN = mean, fill = NA,
align = "right")
# Drop rows with NA values
df_eq <- na.omit(df_eq)
summary(df_eq)
filtered_rows <- df_eq[df_eq$time_diff_float > 86400, ]
num_filtered_rows <- nrow(filtered_rows)
print(num_filtered_rows)
# Filter rows based on the condition time_diff_float > 86400 (which means time difference greater than 86400 seconds)
filtered_rows <- subset(df_eq, time_diff_float > 86400)
print(filtered_rows)
min_date <- min(df_eq$time)
min_date <- as.Date(min_date)
print(min_date)
max_date <- max(df_eq$time)
max_date <- as.Date(max_date)
print(max_date)
max_date <- max(df_eq$time)
max_date <- as.Date(max_date)
print(typeof(max_date))
start_date <- min(df_eq$time)
# Calculate the number of days between min_date and max_date
number_of_days <- as.integer(difftime(max(df_eq$time), min(df_eq$time), units = "days"))
print(paste("number_of_days:", number_of_days))
date_list <- seq(start_date, by = "days", length.out = number_of_days)
print(date_list[2])
print(class(date_list[2]))
start_date <- min(df_eq$time)
number_of_days <- as.integer(difftime(max(df_eq$time), min(df_eq$time), units = "days"))
print(paste("number_of_days:", number_of_days))
date_list <- seq(start_date, by = "days", length.out = number_of_days)
print(date_list[2])
print(class(date_list[2]))
df_eq
setDT(df_eq)
setkey(df_eq, time)
rownames(df_eq) <- df_eq$time
setkey(df_eq, time)
library(zoo)
# Aggregate data by day
df_eq[, date := as.IDate(time)]
df_daily <- df_eq[, .(
mag_max = max(mag, na.rm = TRUE),
event_count = .N,
mag_mean = mean(mag, na.rm = TRUE),
mag_sum = sum(mag, na.rm = TRUE),
mag_scatter = sd(mag, na.rm = TRUE),
longitude_mean = mean(longitude, na.rm = TRUE),
longitude_std = sd(longitude, na.rm = TRUE),
latitude_mean = mean(latitude, na.rm = TRUE),
latitude_std = sd(latitude, na.rm = TRUE),
depth_mean = mean(depth, na.rm = TRUE),
depth_std = sd(depth, na.rm = TRUE),
time_diff_float_mean = mean(time_diff_float, na.rm = TRUE),
time_diff_float_std = sd(time_diff_float, na.rm = TRUE)
), by = date]
# Calculate the rolling mean for the mag_mean column with a window of 10 days
df_daily[, mag_roll_10 := rollapply(mag_mean, width = 10, FUN = mean, fill = NA, align = "right")]
df_daily
library(data.table)
library(ggplot2)
library(naniar)
df_daily <- as.data.frame(df_daily)
ggplot_missing <- vis_miss(df_daily) +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
labs(title = "Heatmap of Missing Values")
print(ggplot_missing)
ggsave("heatmap_missing_values.png", plot = ggplot_missing, width = 8, height = 5)
setDT(df_daily)
df_daily_clean <- df_daily[date > as.IDate("1972-01-01")]
df_daily_clean[, mag_roll_10 := NULL]
print(df_daily_clean)
df_daily_clean <- as.data.frame(df_daily_clean)
ggplot_missing <- vis_miss(df_daily_clean) +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
labs(title = "Heatmap of Missing Values")
print(ggplot_missing)
ggsave("heatmap_missing_values.png", plot = ggplot_missing, width = 8, height = 5)
library(data.table)
library(data.table)
library(ggplot2)
library(naniar)
df_daily <- as.data.frame(df_daily)
ggplot_missing <- vis_miss(df_daily) +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
labs(title = "Heatmap of Missing Values")
print(ggplot_missing)
ggsave("heatmap_missing_values.png", plot = ggplot_missing, width = 8, height = 5)
ggplot_missing <- vis_miss(df_daily_clean) +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
labs(title = "Heatmap of Missing Values")
print(ggplot_missing)
ggsave("heatmap_missing_values.png", plot = ggplot_missing, width = 8, height = 5)
library(data.table)
setDT(df_daily_clean)
str(df_daily_clean)
summary(df_daily_clean)
library(data.table)
library(zoo)
df_daily_clean <- as.data.frame(df_daily_clean)
df_daily_clean$date <- as.POSIXct(df_daily_clean$date)
df_daily_clean <- df_daily_clean[order(df_daily_clean$date), ]
columns_to_interpolate <- c("mag_max", "event_count", "mag_mean", "mag_sum", "mag_scatter",
"longitude_mean", "longitude_std", "latitude_mean",
"latitude_std", "depth_mean", "depth_std",
"time_diff_float_mean", "time_diff_float_std")
# Perform linear interpolation on specified columns
for (col in columns_to_interpolate) {
df_daily_clean[[col]] <- na.approx(df_daily_clean[[col]], rule = 2)  # rule = 2 handles leading/trailing NAs
}
df_daily_clean$date <- as.Date(df_daily_clean$date)
setDT(df_daily_clean)
if (any(is.na(df_daily_clean))) {
print("Warning: There are still NA values in df_daily_clean after interpolation.")
}
print(df_daily_clean)
df_daily_clean <- df_daily_clean[complete.cases(df_daily_clean), ]
print(df_daily_clean)
df_daily_clean <- as.data.frame(df_daily_clean)
str(df_daily_clean)
df_eq <- df_daily_clean
df_eq_shape <- dim(df_eq)
print(df_eq_shape)
str(df_eq)
label <- vector("integer", length = nrow(df_eq))
cnt <- 0
# Loop through each row and assign labels based on magnitude
for (i in 1:nrow(df_eq)) {
if (df_eq$mag_max[i] > 5.5) {
cnt <- cnt + 1
label[i] <- as.integer(cnt)
} else {
label[i] <- 0
}
}
df_eq$large_eq_label <- label
print(df_eq)
df_eq <- as.data.frame(df_eq)
summary_stats <- summary(df_eq)
summary_stats_transposed <- t(summary_stats)
print(summary_stats_transposed)
str(df_eq)
library(ggplot2)
library(gridExtra)
df_eq <- as.data.frame(df_eq)
plots <- lapply(names(df_eq), function(col) {
ggplot(df_eq, aes_string(x = col)) +
geom_histogram() +
labs(title = col)
})
# Arrange plots in a grid layout
grid.arrange(grobs = plots, ncol = 3)  # Example: Arrange in 3 columns
str(df_eq)
library(corrplot)
df_numeric <- df_eq[, !names(df_eq) %in% "date"]
# Calculate the correlation matrix
df_corr <- cor(df_numeric)
print(df_corr)
corrplot(df_corr, method = "color", type = "upper",
col = colorRampPalette(c("blue", "white", "red"))(100),
tl.col = "black", tl.srt = 45, diag = FALSE)
df_eq <- df_daily_clean
print(head(df_eq, 2))
interpret_dftest <- function(dftest) {
dfoutput <- list(
"Test Statistic" = dftest$statistic,
"p-value" = dftest$p.value,
"Lag Used" = dftest$parameter
)
return(dfoutput)
}
library(tseries)
# Perform Dickey-Fuller test
dftest <- adf.test(df_eq$mag_max)
result <- interpret_dftest(dftest)
print(result)
test_size <- 0.25
num_test_rows <- round(nrow(df_eq) * test_size)
test_indices <- sample(seq_len(nrow(df_eq)), size = num_test_rows, replace = FALSE)
# Create the test set using the sampled indices
test <- df_eq[test_indices, ]
# Create the train set by excluding the rows in the test set
train <- df_eq[-test_indices, ]
# Load the vars package
library(vars)
train_ts <- ts(train, frequency = 1)
# Instantiate a VAR model
model <- VAR(train_ts)
# Fit a VAR model with specified parameters
ts_model <- VAR(train_ts, p = 60, type = "const", ic = "aic")
ts_model.k_ar <- ts_model$p
library(httr)
library(jsonlite)
library(dplyr)
library(ggplot2)
library(leaflet)
library(geosphere)
library(maps)
library(mapproj)
library(caret)
library(cluster)
library(datasets)
library(dbscan)
library(ggplot2)
library(Rtsne)
library(randomForest)
library(caret)
library(IRdisplay)
library(repr)
library(xts)
options(repr.plot.width = 6, repr.plot.height = 4)  # Adjust width and height as needed
file_path <- "C:\\Users\\Akhil_kamuni\\OneDrive\\Desktop\\MP_2\\MP_2\\datasets\\combined_eq_california_timeseries.csv"
df_eq <- read.csv(file_path)
df_eq$time <- as.POSIXct(df_eq$time, format = "%Y-%m-%d %H:%M:%S")
head(df_eq, 3)
df_eq_large <- subset(df_eq, mag > 6)
df_eq_large$time <- as.POSIXct(df_eq_large$time, format = "%Y-%m-%d %H:%M:%S")
df_eq_large$time_diff_day <- c(NA, diff(df_eq_large$time)) / (24 * 60 * 60)  # Assuming time difference in seconds
head(df_eq_large, 2)
df_eq$time <- as.Date(df_eq$time)
start_date <- as.Date("1989-09-19")
end_date <- as.Date("1989-10-19")
df_eq_sel_range <- subset(df_eq, time >= start_date & time <= end_date)
head(df_eq_sel_range, 2)
map <- leaflet() %>%
addTiles() %>%
setView(lng = -120, lat = 38, zoom = 5)
map <- map %>%
addCircleMarkers(data = df_eq_sel_range,
lng = ~longitude,
lat = ~latitude,
radius = ~mag/5,
color = "red",
opacity = 0.5)
map
df_eq_sel_range$time <- as.POSIXct(df_eq_sel_range$time)
ss <- scale(df)
eps_param <- 0.33
min_sample <- 7
dbscan <- dbscan(ss, eps = eps_param, minPts = min_sample)
df$cluster <- dbscan$cluster
library(httr)
library(jsonlite)
library(dplyr)
library(ggplot2)
library(leaflet)
library(geosphere)
library(maps)
library(mapproj)
library(caret)
library(cluster)
library(datasets)
library(dbscan)
library(ggplot2)
library(Rtsne)
library(randomForest)
library(caret)
library(IRdisplay)
library(repr)
library(xts)
options(repr.plot.width = 6, repr.plot.height = 4)  # Adjust width and height as needed
file_path <- "C:\\Users\\Akhil_kamuni\\OneDrive\\Desktop\\MP_2\\MP_2\\datasets\\combined_eq_california_timeseries.csv"
df_eq <- read.csv(file_path)
df_eq$time <- as.POSIXct(df_eq$time, format = "%Y-%m-%d %H:%M:%S")
head(df_eq, 3)
df_eq_large <- subset(df_eq, mag > 6)
df_eq_large$time <- as.POSIXct(df_eq_large$time, format = "%Y-%m-%d %H:%M:%S")
df_eq_large$time_diff_day <- c(NA, diff(df_eq_large$time)) / (24 * 60 * 60)  # Assuming time difference in seconds
head(df_eq_large, 2)
df_eq$time <- as.Date(df_eq$time)
start_date <- as.Date("1989-09-19")
end_date <- as.Date("1989-10-19")
df_eq_sel_range <- subset(df_eq, time >= start_date & time <= end_date)
head(df_eq_sel_range, 2)
map <- leaflet() %>%
addTiles() %>%
setView(lng = -120, lat = 38, zoom = 5)
map <- map %>%
addCircleMarkers(data = df_eq_sel_range,
lng = ~longitude,
lat = ~latitude,
radius = ~mag/5,
color = "red",
opacity = 0.5)
map
df_eq_sel_range$time <- as.POSIXct(df_eq_sel_range$time)
ggplot(data = df_eq_sel_range, aes(x = time, y = longitude)) +
geom_line(color = "red") +
geom_point(color = "red") +
labs(title = "Longitude", x = "Time", y = "Longitude") +
theme_minimal() +
theme(plot.title = element_text(hjust = 0.5))
df <- df_eq_sel_range[, c("longitude", "latitude", "depth")]
ss <- scale(df)
eps_param <- 0.33
min_sample <- 7
dbscan <- dbscan(ss, eps = eps_param, minPts = min_sample)
df$cluster <- dbscan$cluster
core_samples_mask <- dbscan$core$border
n_clusters <- length(unique(dbscan$cluster)) - (1 %in% dbscan$cluster)
n_noise <- sum(dbscan$cluster == 0)
cat("Total number of points:", nrow(df), "\n")
cat("Estimated number of clusters:", n_clusters, "\n")
cat("Estimated number of noise points:", n_noise, "\n")
cluster_counts <- table(df$cluster)
cat("Clusters:", cluster_counts, "\n")
top_cluster <- names(sort(table(df$cluster), decreasing = TRUE))[1]
cat("Top cluster is:", top_cluster, "\n")
if (top_cluster == "-1") {
cluster_mask <- names(sort(table(df$cluster), decreasing = TRUE))[2]
} else {
cluster_mask <- top_cluster
}
df_filtered <- df[df$cluster != -1, ]
plot <- ggplot(df_filtered, aes(x = longitude, y = latitude, color = factor(cluster))) +
geom_point(size = 3) +
scale_color_manual(values = c("gray", "blue", "green", "red", "orange","pink","black")) +
labs(title = "Clusters", x = "Longitude", y = "Latitude") +
theme_minimal()
plot
cluster_counts <- table(df$cluster)
sorted_clusters <- sort(cluster_counts, decreasing = TRUE)
top_cluster <- names(sorted_clusters)[1]
print(top_cluster)
print(head(core_samples_mask, 10))
cluster_counts <- table(df$cluster)
print(cluster_counts)
df_top_cluster <- df_eq_sel_range[df_eq_sel_range$cluster == cluster_mask, ]
df_top_cluster
map <- leaflet() %>%
addTiles() %>%
setView(lng = -120, lat = 38, zoom = 5)
map <- map %>%
addCircleMarkers(data = df_top_cluster,
lng = ~longitude,
lat = ~latitude,
radius = ~mag/0.05,
color = "red",
opacity = 0.5)
map
dbscan <- dbscan(ss, eps = eps_param, minPts = min_sample)
labels <- dbscan$cluster
df$cluster_labels <- factor(labels)
plot <- ggplot(df, aes(x = longitude, y = latitude, color = cluster_labels)) +
geom_point(size = 3) +
scale_color_manual(values = c("gray", "blue", "green", "red", "orange")) +
labs(title = "Clusters", x = "Longitude", y = "Latitude") +
theme_minimal()
plot + guides(color = guide_legend(title = "Cluster"))
df_filtered <- df[df$cluster != -1, ]
unique_clusters <- unique(df_filtered$cluster)
print(unique_clusters)
eps_param <- 0.33
print(eps_param)
library(FNN)  # For k-nearest neighbors
nn <- get.knn(ss, k = min_sample)
distances <- nn$nn.dist
sorted_distances <- sort(distances[, min_sample], decreasing = FALSE)
indices <- seq_along(sorted_distances)
relative_change <- c(0, diff(sorted_distances) / sorted_distances[-length(sorted_distances)])
smoothed_change <- stats::filter(relative_change, rep(1/3, 3), sides=2)
# Find knee point
knee_index <- which.max(smoothed_change)
# Plot knee point
plot(indices, sorted_distances, type = "l", xlab = "Index", ylab = "Distances")
abline(v = knee_index, col = "red")
print(sorted_distances[knee_index])
plot <- ggplot(df_filtered, aes(x = longitude, y = latitude, color = factor(cluster))) +
geom_point(size = 3) +
scale_color_manual(values = c("gray", "blue", "green", "red", "orange","pink","black")) +
labs(title = "Clusters", x = "Longitude", y = "Latitude") +
theme_minimal()
plot
